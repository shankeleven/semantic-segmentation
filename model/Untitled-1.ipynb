{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "'''\n",
    "    Sentences are represented as a sequence of word tokens. These sequences are converted into fixed-length sentence embeddings by passing them through a Bi-LSTM. The hidden embedding at the last word is considered to represent the entire sentence.   \n",
    "'''\n",
    "class LSTM_Sentence_Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, drop = 0.5, device = 'cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim // 2, bidirectional = True, batch_first = True)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.hidden = None\n",
    "        self.device = device        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device), torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device))\n",
    "    \n",
    "    def forward(self, sentences, sent_lengths):\n",
    "        ## sentences: tensor[batch_size, max_sent_len]\n",
    "        ## sent_lengths: list[batch_size]        \n",
    "        \n",
    "        # initialize hidden state\n",
    "        batch_size = sentences.shape[0]\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # convert word tokens to word embeddings\n",
    "        ## tensor[batch_size, max_sent_len] --> tensor[batch_size, max_sent_len, emb_dim] \n",
    "        x = self.emb(sentences)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # generate sentence embedding from sequence of word embeddings\n",
    "        ## tensor[batch_size, max_sent_len, emb_dim] --> tensor[2, batch_size, hidden_dim // 2]\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, list(sent_lengths), batch_first = True, enforce_sorted = False)\n",
    "        _, (x, _) = self.lstm(x, self.hidden)\n",
    "\n",
    "        ## tensor[2, batch_size, hidden_dim // 2] --> tensor[batch_size, hidden_dim]        \n",
    "        x = x.permute(1, 0, 2).contiguous().view(batch_size, -1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "'''\n",
    "    A Bi-LSTM is used to generate feature vectors for each sentence from the sentence embeddings. The feature vectors are actually context-aware sentence embeddings. These are then fed to a feed-forward network to obtain emission scores for each class at each sentence.\n",
    "'''\n",
    "class LSTM_Emitter(nn.Module):\n",
    "    def __init__(self, n_tags, emb_dim, hidden_dim, drop = 0.5, device = 'cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim // 2, bidirectional = True, batch_first = True)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, n_tags)\n",
    "        self.hidden = None\n",
    "        self.device = device\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device), torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device))\n",
    "    \n",
    "    def forward(self, sequences):\n",
    "        ## sequences: tensor[batch_size, max_seq_len, emb_dim]\n",
    "        \n",
    "        # initialize hidden state\n",
    "        self.hidden = self.init_hidden(sequences.shape[0])\n",
    "        \n",
    "        # generate context-aware sentence embeddings (feature vectors)\n",
    "        ## tensor[batch_size, max_seq_len, emb_dim] --> tensor[batch_size, max_seq_len, hidden_dim]\n",
    "        x, self.hidden = self.lstm(sequences, self.hidden)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # generate emission scores for each class at each sentence\n",
    "        # tensor[batch_size, max_seq_len, hidden_dim] --> tensor[batch_size, max_seq_len, n_tags]\n",
    "        x = self.hidden2tag(x)\n",
    "        return x\n",
    "    \n",
    "'''\n",
    "    A linear-chain CRF is fed with the emission scores at each sentence, and it finds out the optimal sequence of tags by learning the transition scores.\n",
    "'''\n",
    "class CRF(nn.Module):    \n",
    "    def __init__(self, n_tags, sos_tag_idx, eos_tag_idx, pad_tag_idx = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_tags = n_tags\n",
    "        self.SOS_TAG_IDX = sos_tag_idx\n",
    "        self.EOS_TAG_IDX = eos_tag_idx\n",
    "        self.PAD_TAG_IDX = pad_tag_idx\n",
    "        \n",
    "        self.transitions = nn.Parameter(torch.empty(self.n_tags, self.n_tags))\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # initialize transitions from random uniform distribution between -0.1 and 0.1\n",
    "        nn.init.uniform_(self.transitions, -0.1, 0.1)\n",
    "        \n",
    "        # enforce constraints (rows = from, cols = to) with a big negative number.\n",
    "        # exp(-1000000) ~ 0\n",
    "        \n",
    "        # no transitions to SOS\n",
    "        self.transitions.data[:, self.SOS_TAG_IDX] = -1000000.0\n",
    "        # no transition from EOS\n",
    "        self.transitions.data[self.EOS_TAG_IDX, :] = -1000000.0\n",
    "        \n",
    "        if self.PAD_TAG_IDX is not None:\n",
    "            # no transitions from pad except to pad\n",
    "            self.transitions.data[self.PAD_TAG_IDX, :] = -1000000.0\n",
    "            self.transitions.data[:, self.PAD_TAG_IDX] = -1000000.0\n",
    "            # transitions allowed from end and pad to pad\n",
    "            self.transitions.data[self.PAD_TAG_IDX, self.EOS_TAG_IDX] = 0.0\n",
    "            self.transitions.data[self.PAD_TAG_IDX, self.PAD_TAG_IDX] = 0.0\n",
    "            \n",
    "    def forward(self, emissions, tags, mask = None):\n",
    "        ## emissions: tensor[batch_size, seq_len, n_tags]\n",
    "        ## tags: tensor[batch_size, seq_len]\n",
    "        ## mask: tensor[batch_size, seq_len], indicates valid positions (0 for pad)\n",
    "        return -self.log_likelihood(emissions, tags, mask = mask)\n",
    "    \n",
    "    def log_likelihood(self, emissions, tags, mask = None):                   \n",
    "        if mask is None:\n",
    "            mask = torch.ones(emissions.shape[:2])\n",
    "            \n",
    "        scores = self._compute_scores(emissions, tags, mask = mask)\n",
    "        partition = self._compute_log_partition(emissions, mask = mask)\n",
    "        return torch.sum(scores - partition)\n",
    "    \n",
    "    # find out the optimal tag sequence using Viterbi Decoding Algorithm\n",
    "    def decode(self, emissions, mask = None):      \n",
    "        if mask is None:\n",
    "            mask = torch.ones(emissions.shape[:2])\n",
    "            \n",
    "        scores, sequences = self._viterbi_decode(emissions, mask)\n",
    "        return scores, sequences\n",
    "    \n",
    "    def _compute_scores(self, emissions, tags, mask):\n",
    "        batch_size, seq_len = tags.shape\n",
    "        scores = torch.zeros(batch_size).cuda()\n",
    "        \n",
    "        # save first and last tags for later\n",
    "        first_tags = tags[:, 0]\n",
    "        last_valid_idx = mask.int().sum(1) - 1\n",
    "        last_tags = tags.gather(1, last_valid_idx.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        # add transition from SOS to first tags for each sample in batch\n",
    "        t_scores = self.transitions[self.SOS_TAG_IDX, first_tags]\n",
    "        \n",
    "        # add emission scores of the first tag for each sample in batch\n",
    "        e_scores = emissions[:, 0].gather(1, first_tags.unsqueeze(1)).squeeze()\n",
    "        scores += e_scores + t_scores\n",
    "        \n",
    "        # repeat for every remaining word\n",
    "        for i in range(1, seq_len):\n",
    "            \n",
    "            is_valid = mask[:, i]\n",
    "            prev_tags = tags[:, i - 1]\n",
    "            curr_tags = tags[:, i]\n",
    "            \n",
    "            e_scores = emissions[:, i].gather(1, curr_tags.unsqueeze(1)).squeeze()\n",
    "            t_scores = self.transitions[prev_tags, curr_tags]\n",
    "                        \n",
    "            # apply the mask\n",
    "            e_scores = e_scores * is_valid\n",
    "            t_scores = t_scores * is_valid\n",
    "            \n",
    "            scores += e_scores + t_scores\n",
    "            \n",
    "        # add transition from last tag to EOS for each sample in batch\n",
    "        scores += self.transitions[last_tags, self.EOS_TAG_IDX]\n",
    "        return scores\n",
    "    \n",
    "    # compute the partition function in log-space using forward algorithm\n",
    "    def _compute_log_partition(self, emissions, mask):\n",
    "        batch_size, seq_len, n_tags = emissions.shape\n",
    "        \n",
    "        # in the first step, SOS has all the scores\n",
    "        alphas = self.transitions[self.SOS_TAG_IDX, :].unsqueeze(0) + emissions[:, 0]\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "            ## tensor[batch_size, n_tags] -> tensor[batch_size, 1, n_tags]\n",
    "            e_scores = emissions[:, i].unsqueeze(1) \n",
    "            \n",
    "            ## tensor[n_tags, n_tags] -> tensor[batch_size, n_tags, n_tags]\n",
    "            t_scores = self.transitions.unsqueeze(0)\n",
    "            \n",
    "            ## tensor[batch_size, n_tags] -> tensor[batch_size, n_tags, 1]\n",
    "            a_scores = alphas.unsqueeze(2)\n",
    "            \n",
    "            scores = e_scores + t_scores + a_scores\n",
    "            new_alphas = torch.logsumexp(scores, dim = 1)\n",
    "            \n",
    "            # set alphas if the mask is valid, else keep current values\n",
    "            is_valid = mask[:, i].unsqueeze(-1)\n",
    "            alphas = is_valid * new_alphas + (1 - is_valid) * alphas\n",
    "            \n",
    "        # add scores for final transition\n",
    "        last_transition = self.transitions[:, self.EOS_TAG_IDX]\n",
    "        end_scores = alphas + last_transition.unsqueeze(0)\n",
    "        \n",
    "        # return log_sum_exp\n",
    "        return torch.logsumexp(end_scores, dim = 1)\n",
    "    \n",
    "    # return a list of optimal tag sequence for each example in the batch\n",
    "    def _viterbi_decode(self, emissions, mask):\n",
    "        batch_size, seq_len, n_tags = emissions.shape\n",
    "        \n",
    "        # in the first iteration, SOS will have all the scores and then, the max\n",
    "        alphas = self.transitions[self.SOS_TAG_IDX, :].unsqueeze(0) + emissions[:, 0]\n",
    "        \n",
    "        backpointers = []\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "            ## tensor[batch_size, n_tags] -> tensor[batch_size, 1, n_tags]\n",
    "            e_scores = emissions[:, i].unsqueeze(1) \n",
    "            \n",
    "            ## tensor[n_tags, n_tags] -> tensor[batch_size, n_tags, n_tags]\n",
    "            t_scores = self.transitions.unsqueeze(0)\n",
    "            \n",
    "            ## tensor[batch_size, n_tags] -> tensor[batch_size, n_tags, 1]\n",
    "            a_scores = alphas.unsqueeze(2)\n",
    "            \n",
    "            scores = e_scores + t_scores + a_scores\n",
    "            \n",
    "            # find the highest score and tag, instead of log_sum_exp\n",
    "            max_scores, max_score_tags = torch.max(scores, dim = 1)\n",
    "            \n",
    "            # set alphas if the mask is valid, otherwise keep the current values\n",
    "            is_valid = mask[:, i].unsqueeze(-1)\n",
    "            alphas = is_valid * max_scores + (1 - is_valid) * alphas\n",
    "            \n",
    "            backpointers.append(max_score_tags.t())\n",
    "            \n",
    "        # add scores for final transition\n",
    "        last_transition = self.transitions[:, self.EOS_TAG_IDX]\n",
    "        end_scores = alphas + last_transition.unsqueeze(0)\n",
    "\n",
    "        # get the final most probable score and the final most probable tag\n",
    "        max_final_scores, max_final_tags = torch.max(end_scores, dim=1)\n",
    "\n",
    "        # find the best sequence of labels for each sample in the batch\n",
    "        best_sequences = []\n",
    "        emission_lengths = mask.int().sum(dim=1)\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            # recover the original sentence length for the i-th sample in the batch\n",
    "            sample_length = emission_lengths[i].item()\n",
    "\n",
    "            # recover the max tag for the last timestep\n",
    "            sample_final_tag = max_final_tags[i].item()\n",
    "\n",
    "            # limit the backpointers until the last but one\n",
    "            # since the last corresponds to the sample_final_tag\n",
    "            sample_backpointers = backpointers[: sample_length - 1]\n",
    "\n",
    "            # follow the backpointers to build the sequence of labels\n",
    "            sample_path = self._find_best_path(i, sample_final_tag, sample_backpointers)\n",
    "\n",
    "            # add this path to the list of best sequences\n",
    "            best_sequences.append(sample_path)\n",
    "\n",
    "        return max_final_scores, best_sequences\n",
    "    \n",
    "    # auxiliary function to find the best path sequence for a specific example\n",
    "    def _find_best_path(self, sample_id, best_tag, backpointers):\n",
    "        ## backpointers: list[tensor[seq_len_i - 1, n_tags, batch_size]], seq_len_i is the length of the i-th sample of the batch\n",
    "        \n",
    "        # add the final best_tag to our best path\n",
    "        best_path = [best_tag]\n",
    "\n",
    "        # traverse the backpointers in backwards\n",
    "        for backpointers_t in reversed(backpointers):\n",
    "\n",
    "            # recover the best_tag at this timestep\n",
    "            best_tag = backpointers_t[best_tag][sample_id].item()\n",
    "\n",
    "            # append to the beginning of the list so we don't need to reverse it later\n",
    "            best_path.insert(0, best_tag)\n",
    "\n",
    "        return best_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "    Top-level module which uses a Hierarchical-LSTM-CRF to classify.\n",
    "    \n",
    "    If pretrained = False, each example is represented as a sequence of sentences, which themselves are sequences of word tokens. Individual sentences are passed to LSTM_Sentence_Encoder to generate sentence embeddings. \n",
    "    If pretrained = True, each example is represented as a sequence of fixed-length pre-trained sentence embeddings.\n",
    "    \n",
    "    Sentence embeddings are then passed to LSTM_Emitter to generate emission scores, and finally CRF is used to obtain optimal tag sequence. \n",
    "    Emission scores are fed to the CRF to generate optimal tag sequence.\n",
    "'''\n",
    "class Hier_LSTM_CRF_Classifier(nn.Module):\n",
    "    def __init__(self, n_tags, sent_emb_dim, sos_tag_idx, eos_tag_idx, pad_tag_idx, vocab_size = 0, word_emb_dim = 0, pad_word_idx = 0, pretrained = False, device = 'cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = sent_emb_dim\n",
    "        self.pretrained = pretrained\n",
    "        self.device = device\n",
    "        self.pad_tag_idx = pad_tag_idx\n",
    "        self.pad_word_idx = pad_word_idx\n",
    "        \n",
    "        # sentence encoder is not required for pretrained embeddings\n",
    "        self.sent_encoder = LSTM_Sentence_Encoder(vocab_size, word_emb_dim, sent_emb_dim).to(self.device) if not self.pretrained else None\n",
    "            \n",
    "        self.emitter = LSTM_Emitter(n_tags, sent_emb_dim, sent_emb_dim).to(self.device)\n",
    "        self.crf = CRF(n_tags, sos_tag_idx, eos_tag_idx, pad_tag_idx).to(self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = len(x)\n",
    "        seq_lengths = [len(doc) for doc in x]\n",
    "        max_seq_len = max(seq_lengths)\n",
    "        \n",
    "        if not self.pretrained: ## x: list[batch_size, sents_per_doc, words_per_sent]\n",
    "            tensor_x = []\n",
    "            for doc in x:\n",
    "                sents = [torch.tensor(s, dtype = torch.long) for s in doc]\n",
    "                sent_lengths = [len(s) for s in doc]\n",
    "                \n",
    "                ## list[sents_per_doc, words_per_sent] --> tensor[sents_per_doc, max_sent_len]\n",
    "                sents = nn.utils.rnn.pad_sequence(sents, batch_first = True, padding_value = self.pad_word_idx).to(self.device)\n",
    "                \n",
    "                ## tensor[sents_per_doc, max_sent_len] --> tensor[sents_per_doc, sent_emb_dim]\n",
    "                sents = self.sent_encoder(sents, sent_lengths)\n",
    "        \n",
    "                tensor_x.append(sents)\n",
    "            \n",
    "        else: ## x: list[batch_size, sents_per_doc, sent_emb_dim]\n",
    "            tensor_x = [torch.tensor(doc, dtype = torch.float, requires_grad = True) for doc in x]\n",
    "        \n",
    "        ## list[batch_size, sents_per_doc, sent_emb_dim] --> tensor[batch_size, max_seq_len, sent_emb_dim]\n",
    "        tensor_x = nn.utils.rnn.pad_sequence(tensor_x, batch_first = True).to(self.device)        \n",
    "        \n",
    "        self.mask = torch.zeros(batch_size, max_seq_len).to(self.device)\n",
    "        for i, sl in enumerate(seq_lengths):\n",
    "            self.mask[i, :sl] = 1\t\n",
    "        \n",
    "        self.emissions = self.emitter(tensor_x)\n",
    "        _, path = self.crf.decode(self.emissions, mask = self.mask)\n",
    "        return path\n",
    "    \n",
    "    def _loss(self, y):\n",
    "        ##  list[batch_size, sents_per_doc] --> tensor[batch_size, max_seq_len]\n",
    "        tensor_y = [torch.tensor(doc, dtype = torch.long) for doc in y]\n",
    "        tensor_y = nn.utils.rnn.pad_sequence(tensor_y, batch_first = True, padding_value = self.pad_tag_idx).to(self.device)\n",
    "        \n",
    "        nll = self.crf(self.emissions, tensor_y, mask = self.mask)\n",
    "        return nll    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
